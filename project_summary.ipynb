{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Game\n",
    "from investigate_game import InvestigateGame\n",
    "from random_player import RandomPlayer\n",
    "from min_max import MinMaxPlayer, AlphaBetaMinMaxPlayer\n",
    "from q_learning import QLearningRLPlayer\n",
    "from monte_carlo import MonteCarloRLPlayer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_stats(rewards: list, step_size: int, switch_ratio: int | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Given a training list of rewards, this function plots the averages of the rewards\n",
    "    using a given step size.\n",
    "\n",
    "    Args:\n",
    "        rewards: the rewards achieved during training time;\n",
    "        step: the step size to be used to plot an average value;\n",
    "        switch_ratio: define the moment in which minmax started playing-\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # https://stackoverflow.com/questions/15956309/averaging-over-every-n-elements-of-a-numpy-array\n",
    "    # compute the averages\n",
    "    averages = np.mean(np.array(rewards).reshape(-1, step_size), axis=1)\n",
    "\n",
    "    # define the width and height of the figure in inches\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # plot the averages\n",
    "    plt.plot(averages, color='red', marker='o', linestyle='dashed', label='Training')\n",
    "    # if minmax played\n",
    "    if switch_ratio is not None:\n",
    "        # compute the exact moment\n",
    "        switch_moment = int(len(rewards) / step_size * switch_ratio)\n",
    "        # print a vertical line at the moment minmax started playing\n",
    "        plt.axvline(x=switch_moment, color='green', label='MinMax Starts Playing')\n",
    "    # specify the title\n",
    "    plt.title('Training Summary')\n",
    "    # specify the x-axis label\n",
    "    plt.xlabel('# of the step')\n",
    "    # specify the y-axis label\n",
    "    plt.ylabel('Mean rewards value')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Q-learning player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Q-learning player\n",
    "q_learning_rl_agent = QLearningRLPlayer(\n",
    "    n_episodes=20_000,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay_rate=1e-4,\n",
    "    minmax=True,\n",
    ")\n",
    "# train the Q-learning player\n",
    "q_learning_rl_agent.train(max_steps_draw=10)\n",
    "# print the number of explored states\n",
    "print(f'Number of explored states: {len(q_learning_rl_agent._q_table.keys())}')\n",
    "# serialize the Q-learning player\n",
    "q_learning_rl_agent.save('agents/q_learning_agent_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rewards\n",
    "rewards = q_learning_rl_agent.rewards\n",
    "# plot the averages of the training rewards\n",
    "plot_training_stats(rewards, step_size=5, switch_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Monte Carlo learning player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Q-learning player\n",
    "monte_carlo_rl_agent = MonteCarloRLPlayer(\n",
    "    n_episodes=20_000,\n",
    "    gamma=0.99,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay_rate=1e-4,\n",
    "    minmax=True,\n",
    ")\n",
    "# train the Q-learning player\n",
    "monte_carlo_rl_agent.train(max_steps_draw=10)\n",
    "# print the number of explored states\n",
    "print(f'Number of explored states: {len(monte_carlo_rl_agent._q_table.keys())}')\n",
    "# serialize the Q-learning player\n",
    "monte_carlo_rl_agent.save('agents/monte_carlo_rl_agent_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the rewards\n",
    "rewards = monte_carlo_rl_agent.rewards\n",
    "# plot the averages of the training rewards\n",
    "plot_training_stats(rewards, step_size=100, switch_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for k in monte_carlo_rl_agent._q_table.keys():\n",
    "    for v in monte_carlo_rl_agent._q_table[k].values():\n",
    "        v = abs(v)\n",
    "        if v != 10.0 and v != 0.0:\n",
    "            counter += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the first player\n",
    "player1 = MinMaxPlayer(player_id=0, depth=1, symmetries=False)\n",
    "# create the second player\n",
    "player2 = monte_carlo_rl_agent\n",
    "# create a new game\n",
    "game = InvestigateGame(Game())\n",
    "# LET'S BATTLE\n",
    "winner = game.play(player1, player2, max_steps_draw=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(np.array([3, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game._board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from symmetry import Symmetry\n",
    "\n",
    "player_id = game.get_current_player()\n",
    "new_game, state_repr_index, trasformation_index = monte_carlo_rl_agent._map_state_to_index(game, player_id)\n",
    "# get all possible transitions\n",
    "canonical_actions, _ = zip(*new_game.generate_possible_transitions(player_id))\n",
    "# if the current state is known\n",
    "if state_repr_index in monte_carlo_rl_agent._q_table:\n",
    "    # take the action with maximum return of rewards\n",
    "    canonical_action = max(canonical_actions, key=lambda a: monte_carlo_rl_agent._q_table[state_repr_index][a])\n",
    "else:\n",
    "    # choose a random action\n",
    "    canonical_action = choice(canonical_actions)\n",
    "\n",
    "# get action for original state by mapping\n",
    "action = Symmetry.get_action_from_canonical_action(canonical_action, trasformation_index)\n",
    "\n",
    "# return the action\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game._board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(canonical_actions, key=lambda a: monte_carlo_rl_agent._q_table[state_repr_index][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.rot90(np.flipud(game._board), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Symmetry.get_transformed_states(game)[trasformation_index]._board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_game._board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trasformation_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_rl_agent = MonteCarloRLPlayer(\n",
    "    n_episodes=20_000,\n",
    "    gamma=0.99,\n",
    "    min_exploration_rate=0.01,\n",
    "    exploration_decay_rate=1e-4,\n",
    "    minmax=True,\n",
    ")\n",
    "# serialize the Q-learning player\n",
    "monte_carlo_rl_agent.load('agents/monte_carlo_rl_agent_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Symmetry.canonical_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flipud(Symmetry.canonical_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Symmetry.trasformed_positions[4][(3, 4)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
